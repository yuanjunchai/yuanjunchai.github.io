---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Hi! Here is Yuanjun Chai æŸ´æºå› (sounds like Y-wen Joon, Ch-eye), aka Allen. Now I am a MSEE student in University of Washington Seattle<img src='./images/UWlogo.png' style='width: 2.0em;'> (Go Husky!). 

<!-- Hi! Here is Yuanjun Chai æŸ´æºå› (sounds like Y-wen Joon, Ch-eye), aka Allen. I work at [iFlyTek AI Lab](https://xinghuo.xfyun.cn/sparkapi) <img src='./images/iFlyTek_logo.jpeg' style='width: 3.8em;'>, working on LLM and Multi-LLM. -->
<!-- <br /> -->
<span style="line-height:1.5;">

I graduated with highest honors from Xidian University, earning a bachelor's degree. My thesis about image inpainting received invaluable support by [Chao Dong](https://scholar.google.com/citations?user=OSDCB0UAAAAJ&hl=en&oi=ao) from SIAT-CAS <img src='./images/siat.jpeg' style='width: 1.6em;'>. Also, I am so lucky to have the privilege of collaborating with [Chao Dong](https://scholar.google.com/citations?user=OSDCB0UAAAAJ&hl=en&oi=ao) and [Yu Qiao](https://scholar.google.com/citations?hl=en&user=gFtI-8QAAAAJ) from CAS <img src='./images/cas.jpeg' style='width: 1.6em;'> working on image&video super-resolution,[Yue Gao](https://www.gaoyue.org/)  from Tsinghua University <img src='./images/tsinghua.png' style='width: 1.6em;'> working on event-camera.<br /> 
 <!-- [Jason Cheung](https://www.ortho.hku.hk/biography/cheung-pui-yin-jason/) from HKU <img src='./images/hku.png' style='width: 1.6em;'> working on AI healthcare,  -->

<!-- and [Eric Yi](https://scholar.google.com/citations?user=UyZL660AAAAJ&hl=en) -->
<span style="line-height:1.5;">
Previously, I worked as a machine learning engineer for over 3 years at IT companies, such as [VMware AI Lab](https://www.vmware.com/artificial-intelligence/ai-labs.html) <img src='./images/vmwarelogo.jpeg' style='width: 2.8em; vertical-align:middle;'>, focusing on LLM, agent, and RAG. Before this, I worked in [YeahMobi](https://en.yeahmobi.com/)â€”an affiliate of [Alibaba Group](https://www.alibabagroup.com/en-US) <img src='./images/Alibaba-Logo.png' style='width: 3.0em; vertical-align:middle;'>â€”as a machine learning scientist. I was responsible for all technical development of the AIGC platform [Kreado AI](https://kreadoai.com/) <img src='./images/kreado_logo.png' style='width: 2.8em; vertical-align:middle;'>, including video creation, virtual avatar, and more.
</span> 

<!-- <img src='./images/SparkLLM.png' style='width: 2.7em;'>, working on SparkLLM, RAG, agent and Multi-modal LLM. -->
<!-- Then, I am currently a machine learning engineer at VMware AI Lab <img src='./images/vmware.png' style='width: 2.3em;'>, working on LLM agent, RAG and VLM.<br /> -->
<!-- Then, I decide to go back to UW to continue my research journey. -->
### Research Interests:

<!-- - **Generative AI**: LLM in education, LLM diversity, Multi-modal LLM, text-to-image, text-to-video -->
<!-- - **Virtual Avatar**: 2D&3D talking face generation, voice generation, custom character -->
<!-- - **AI in healthcare**: medical image, medical VLM -->
- **Computer Vision**: Low&High level vision, 3D vision, Vision-Language Model (VLM)
- **NLP**: LM agent, RLVR, LLM diversity
- **Embodiment**: Vision-Language Action Model (VLA)

<!-- By the way, with strong research and industry experience, I decide to **seek for a PhD position**.  -->
Not only diving into research, I am also willing to empower new technologies into products to **make people's lives better**. Thus, I co-founded a start-up [INGREM inc](https://gazerecorder.com/gazepointer/), **to help high-paraplegia disabled people** using computer with precise eyes controling platform. 

<!-- By the way, I am open to collaborate with other researchers. Although pursuing academics is a tough journey, but we could overcome it together. Now I am holding multiple GPUs. If you are interested in my research, please directly contact me: [yuanjunchai89@gmail.com](mailto:yuanjunchai89@gmail.com). -->


<!-- My research interest includes neural machine translation and computer vision. I have published more than 100 papers at the top international AI conferences with total <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'>google scholar citations <strong><span id='total_cit'>260000+</span></strong></a> (You can also use google scholar badge <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>). -->


# ğŸ”¥ News
<!-- - *2023.02*: &nbsp;ğŸ‰ğŸ‰ Thrilled to join iFlyTek AI Lab as MLE! We do some interesting projects on RAG-based LLM like [h2oGPT](https://github.com/h2oai/h2ogpt) (â­ï¸8k+). -->
- *2025.10*: &nbsp;ğŸ‰ğŸ‰ Obtained UW Graduate Student Conference Presentation Award and UW ECE Student Professional Development Award, to support attending ICCV at Hawaii.
- *2025.09*: &nbsp;ğŸ”¥ğŸ”¥ Our paper [Artificial Hivemind](https://github.com/liweijiang/artificial-hivemind) about Large Language Model Homogeneity pattern, has been accepted by **NeurIPS Oral (DB Track)**! See you in San Diego! 
- *2025.06*: &nbsp;ğŸ”¥ğŸ”¥ Our paper [DiffPure-VLM](https://arxiv.org/abs/2504.01308) about Vision-Language Model Safeguarding, has been accepted by **ICCV25**! See you in Hawaii! ğŸ–ï¸
- *2024.09*: &nbsp;ğŸ¥°ğŸ¥° Go to University of Washington! I am so excited to start my research new journey in UW!
- *2022.07*: &nbsp;ğŸ‰ğŸ‰ Thrilled to join VMware as MLE! We do some interesting projects on own LLM platform like [h2oGPT](https://github.com/h2oai/h2ogpt) (â­ï¸8k+).
<!-- - *2022.06*: &nbsp;ğŸ”¥ğŸ”¥ Our AIGC platform [Kreado AI](https://kreadoai.com/) has released to all over the worldï¼ -->
<!-- - *2021.04*: &nbsp;ğŸ‘ğŸ‘ Glad to obtain fully-funded PhD offer from University of HongKong (HKU)! -->
- *2021.03*: &nbsp;ğŸ‘ğŸ‘ Rank 10 / 60 in [NTIRE 2021 Challenge on Image Deblurring](https://competitions.codalab.org/competitions/28073) in **CVPR 2021** and our method **Visual Token Transformer for Image Restoration** is selected to present in the [summary paper](https://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Nah_NTIRE_2021_Challenge_on_Image_Deblurring_CVPRW_2021_paper.pdf).
- *2021.01*: &nbsp;ğŸ¥°ğŸ¥° Our eyes control platform has helped high-paraplegia disabled people more than 300!
- *2020.08*: &nbsp;ğŸ‰ğŸ‰ Our [IKC](https://github.com/yuanjunchai/IKC) -- CVPR project about real-world super-resolution get more than â­ï¸200+.

# ğŸ“ Research 
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS Oral (DB Track)</div><a href="images/LLM_diversity_part.png"><img src='images/LLM_diversity_part.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">
[**Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)**](https://arxiv.org/abs/2510.22954)

Liwei Jiang, **Yuanjun Chai**, Margaret Li, Mickel Liu, Raymond Fok, Maarten Sap, Yulia Tsvetkov, Nouha Dziri, Alon Albalak, Yejin Choi

[**Project**](https://github.com/liweijiang/artificial-hivemind) 
<!-- \| [![GitHub Repo stars](https://github.com/liweijiang/artificial-hivemind)
](https://github.com/liweijiang/artificial-hivemind) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong> -->

- Uncovered and analyzed the "Artificial Hivemind" effect, demonstrating significant intra-model repetition and inter-model homogeneity in current language models. Introduced $\infty$-Chats, the first large-scale (26K queries) dataset of real-world open-ended user queries, and developed a comprehensive taxonomy (6 categories, 17 subcategories) for LLM prompts.
</div>
</div>
<!-- ## Vision-Language Model -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV</div><a href="images/diffpureVLM.png"><img src='images/diffpureVLM.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">
[**Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks**](https://arxiv.org/abs/2504.01308)

Jiawei Wang\*, Yushen Zuo\*, **Yuanjun Chai**, Zhendong Liu, Yicheng Fu, Yichun Feng, Kin-Man Lam

[**Project**](https://github.com/JarvisUSTC/DiffPureRobustVLM) \| [![GitHub Repo stars](https://img.shields.io/github/stars/JarvisUSTC/DiffPure-RobustVLM)
](https://github.com/JarvisUSTC/DiffPureRobustVLM) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>

<!-- - We propose the **Robust-VLGuard dataset** and **DiffPure-VLM** defense framework to address the problem that visual language models (VLMs) are vulnerable to Gaussian noise and adversarial perturbation attacks. Through fine-tuning with Gaussian noise enhancement, the attack success rate of VLMs on datasets such as CIFAR-10 and ImageNet is significantly reduced (for example, the attack success rate of MiniGPT-4 in the RealToxicityPrompts benchmark test dropped from 44.1% to 16.5%). Combined with the distribution conversion capability of the diffusion model DiffPure, adversarial noise is converted into Gaussian noise, further improving the defense effect, especially under strong attacks (for example, the attack success rate of InternVL2 dropped from 57.3% to 36.1% when Ïµ=64/255). -->
<!-- - We notice Vision-Language Models (VLMs) are surprisingly vulnerable to "jailbreak attacks," especially when faced with noisy images or simple Gaussian noise perturbations. To address it, we propose Robust-VLGuard (a new multimodal safety dataset) and DiffPure-VLM (a novel defense framework). -->
- Our **Robust-VLGuard** dataset and **DiffPure-VLM** defense framework tackle the vulnerability of visual language models (VLMs) to adversarial perturbation attacks. By combining Gaussian noise enhancement and diffusion model-based adversarial noise conversion, we demonstrably improve VLM robustness, even against strong attacks.
</div>
</div>

<!-- ## ğŸ‘ï¸ Computer Vision -->
<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR</div><a href="images/ikc_arch02.png"><img src='images/ikc_arch02.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

[**IKC: Blind Super-Resolution With Iterative Kernel Correction**](https://arxiv.org/pdf/1904.03377.pdf)

Jinjin Gu, Hannan Lu, Wangmeng Zuo, Chao Dong

[**Project**](https://www.jasongt.com/projectpages/IKC.html) \| [![](https://img.shields.io/github/stars/yuanjunchai/IKC?style=social&label=Code+Stars)](https://github.com/yuanjunchai/IKC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Our innovative Iterative Kernel Correction (IKC) method tackles blind super-resolution by leveraging characteristic artifacts from kernel mismatch to refine blur kernel estimations. This, combined with our SFTMD network architecture utilizing spatial feature transform layers, delivers enhanced performance across various blur conditions. The code implementation is available on my [GitHub](https://github.com/yuanjunchai/IKC).
</div>
</div>  -->

<!-- - We introduce the Iterative Kernel Correction (IKC) method, which identifies and rectifies blur kernel inaccuracies to achieve superior results in blind super-resolution, further enhanced by our SFTMD network for diverse blur kernels. -->
<!-- - We propose an Iterative Kernel Correction (IKC) method for blur kernel estimation in blind SR problem, where the blur kernels are unknown. We draw the observation that kernel mismatch could bring regular artifacts (either over-sharpening or over-smoothing), which can be applied to correct inaccurate blur kernels. Thus we introduce an iterative correction scheme â€“ IKC that achieves better results than direct kernel estimation. We further propose an effective SR network architecture using spatial feature transform (SFT) layers to handle multiple blur kernels, named SFTMD. -->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPRw</div><a href="images/cvprw.png"><img src='images/cvprw.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

[**NTIRE 2021 Challenge on Image Deblurring**](https://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Nah_NTIRE_2021_Challenge_on_Image_Deblurring_CVPRW_2021_paper.pdf)

Seungjun Nah, Sanghyun Son, Suyoung Lee, Radu Timofte, Kyoung Mu Lee, Yushen Zuo, **Yuanjun Chai** et al.

<!-- [**Project**](https://www.jasongt.com/projectpages/IKC.html) \| [![](https://img.shields.io/github/stars/yuanjunchai/IKC?style=social&label=Code+Stars)](https://github.com/yuanjunchai/IKC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong> -->
- We propose new method **Visual Token Transformer for Image Restoration** for image deblurring at NTIRE 2021 Challenge on Image Deblurring, which achieves the top 10 place in the leaderboard.
</div>
</div> 



# ğŸ–¥ï¸ Industrial Experience
## ğŸ§‘â€ğŸ¨ AIGC (Generative Model)
<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="images/kreado03.png"><img src='images/kreado03.png' alt="sym" width="120%"></a></div></div>
<div class='paper-box-text' markdown="1">

[**Kreado AI: AIGC Platform for Marketing Content Generation**](https://kreadoai.com/)

**Yuanjun Chai** and YeahMobi.inc

- We propose a new AIGC platform for marketing content generation, named Kreado AI. Kreado AI is a hybrid worldwide AIGC platform that combines the strengths of so many AIGC functions:
  - Virtual Avatar (talking-face generation, speech synthesis, LLM)
  - AI model (text-to-image, LoRA, control net)
  - Custom clone serivces (image-to-video, voice clone)
  <!-- - Many AI tools and AI property -->
- Here I mainly focus on the Virtual Avatar and AI model algorithms improvement, as well as collaborate with system architect for entire architecture improvement. Users radiate to Europe, Africa, Southeast Asia, and the Americas, with quarterly revenue exceeding US$1 million.
</div>
</div>

## ğŸ§™â€â™‚ï¸ RAG-based LLM
<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="images/h2oGPT.png"><img src='images/h2oGPT.png' alt="sym" width="120%"></a></div></div>
<div class='paper-box-text' markdown="1">

[**h2o GPT: AIGC Platform for Marketing Content Generation**](https://tanzu.vmware.com/content/blog/vmware-greenplum-excels-as-genai-llm-data-platform)

**Yuanjun Chai**

[**Project**](https://gpt.h2o.ai/) \| [![](https://img.shields.io/github/stars/h2oai/h2ogpt?style=social&label=Code+Stars)](https://github.com/h2oai/h2ogpt) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- We develop a new RAG-based LLM platform for AI cloud native and private AI. The platform could leverage diverse LLMs with extended dataset such as pdf, code base, dataset and internet links. Here I am responsible for all RAG-based LLM algorithm development, as well as industrial deployment. Functionality includes: 
  - text QA and chat with RAG
  - multi-modal QA and chat
  - AI agent
- Next step we would take research about Multi-modal LLM for AI cloud native.
</div>
</div>


## ğŸ‘¨â€âš•ï¸ AI healthcare & Charity
<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="images/eyes02.jpg"><img src='images/eyes02.jpg' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

[**Face Control: Fine Facial Control Platform for High-paraplegia Disabled People**](https://www.bilibili.com/video/BV1u34y1d7g3/?p=1&share_medium=iphone&share_plat=ios&share_session_id=FDFAD6F5-CCF8-47D9-B33E-DE9CB343384E&share_source=WEIXIN&share_tag=s_i&timestamp=1639402219&unique_k=OkjAetO)

**Yuanjun Chai**, Ingrem.inc

- I co-founded a start-up Ingrem, with other hardcore guys. We aim to build up a entire bed for living and playing of high-paraplegia disabled people. Here, I am responsible for the development of the software -- eyes&facial control platform. Based on computer vision algorithms, the system could help the diabled use their face details (such as eyebrow, eye, mouth, etc.) to control mouse and keyborad elaborately. Thus, our platform and our bed entirely enhance the accessibility of normal computer usage and social networks. We do believe tech
 make people's lives better, and we do it!
</div>
</div>

# ğŸ– Honors and Awards
<!-- - *2021.04* Obtain a fully-funded PhD return offer from Li Ka Shing Faculty of Medicine, University of Hong Kong. -->
- Outstanding Undergraduate Student Award. 
- Outstanding Undergraduate Thesis Award (10/5000), Topic: Image Inpainting Based on Deep Learning. 
- Cambridge Summer AI Academic Programme Excellent Student â€“ Full Fulled Scholarship.
- Golden Medal of National Computer Design Contest -- Birdsong Recognition with Machine Learning.

# ğŸ“ Educations
- *2024.09 - now*, Master, University of Washington, Seattle
- *2015.08 - 2019.06*, Undergraduate, Xidian University. 
- *2018.08 - 2018.09*, Summer Exchange Program, University of Cambridge (with Prof. Pietro Lio). 
- *2012.08 - 2015.06*, High School Affiliated to Northwestern University

<!-- # ğŸ’¬ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/) -->

<!-- # ğŸ§‘â€ğŸ’» Professional Experience
- *2023.07 - now*, Machine Learning Engineer, VMware AI Lab
- *2022.07 - 2023.07*, Senior Machine Learning Scientist, YeahMobi -- Alibaba Group.
- *2019.05 - 2022.07*, Research Assistant in Chinese Academy of Sciences, Tsinghua University and HKU (get return PhD offer). -->
# ğŸ§‘â€ğŸ’» Professional Experience
- *2022.07 - 2024.09*, Senior Machine Learning Engineer, VMware AI Lab
<!-- - *2023 - now*, Machine Learning Engineer, iFlyTek -->
- *2020.07 - 2022.07*, Senior Machine Learning Scientist, YeahMobi -- Alibaba Group.
- *2019.05 - 2020.07*, Research Assistant in CAS, Tsinghua University

# ğŸƒâ€â™‚ï¸ Hobbies
My hobbies include FencingğŸ¤º, BasketballğŸ€, SwimmingğŸŠ, GuitarğŸ¸ and MotorcycleğŸï¸. In the high school, I get my first gold medal in FencingğŸ¤º at the National Province GamesğŸ….

Besides, I have my lovely catsğŸ±: 
<br />
<img src='images/cat01.jpg' alt="sym01" width="25%"> <img src='images/cat02.jpg' alt="sym02" width="25%"> <img src='images/cat03.jpg' alt="sym03" width="25%">
